{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main script consists of two parts:\n",
    "* Table creator script: it creates the tables needed. To parse the raw data it uses a custom regex serde.\n",
    "* Query: it performs the query to obtain the number of registers in 2008-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table creator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting table-creater.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile table-creater.hql\n",
    "\n",
    "USE joyvan;\n",
    "\n",
    "DROP TABLE IF EXISTS posts_sample_external;\n",
    "CREATE EXTERNAL TABLE posts_sample_external (\n",
    "Id int,\n",
    "PostTypeId TINYINT,\n",
    "CreationDate string,\n",
    "Tags string,\n",
    "OwnerUserId int,\n",
    "ParentId int,\n",
    "Score int,\n",
    "FavoriteCount int\n",
    ")\n",
    "ROW FORMAT\n",
    "SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "\"input.regex\" = '^<row(?=.*\\\\bId=\"(\\\\d+)\")(?=.*\\\\bPostTypeId=\"(\\\\d)\")(?=.*\\\\bCreationDate=\"(\\\\d{4}-[01]\\\\d-[0-3]\\\\dT[0-2]\\\\d:[0-5]\\\\d:[0-5]\\\\d\\\\.\\\\d+)\")(?=.*\\\\bTags=\"([^\"]+)\")?(?=.*\\\\bOwnerUserId=\"(\\\\d+)\")?(?=.*\\\\bParentId=\"(\\\\d+)\")?(?=.*\\\\bScore=\"(-?\\\\d+)\")(?=.*\\\\bFavoriteCount=\"(\\\\d+)\")?.*$'\n",
    ")\n",
    "LOCATION '/data/stackexchange1000/posts';\n",
    "\n",
    "DROP TABLE IF EXISTS posts_sample;\n",
    "CREATE TABLE posts_sample (\n",
    "    Id int,\n",
    "    PostTypeId int,\n",
    "    CreationDate string,\n",
    "    Tags string,\n",
    "    OwnerUserId int,\n",
    "    ParentId int,\n",
    "    Score int,\n",
    "    FavoriteCount int\n",
    ")\n",
    "PARTITIONED BY (year int, month string);\n",
    "\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "SET hive.exec.max.dynamic.partitions=2048;\n",
    "SET hive.exec.max.dynamic.partitions.pernode=256;\n",
    "SET hive.exec.max.created.files=10000;\n",
    "SET hive.error.on.empty.partition=true;\n",
    "\n",
    "INSERT OVERWRITE TABLE posts_sample PARTITION (year, month)\n",
    "SELECT Id,\n",
    "PostTypeId,\n",
    "CreationDate,\n",
    "Tags,\n",
    "OwnerUserId,\n",
    "ParentId,\n",
    "Score,\n",
    "FavoriteCount,\n",
    "substr(CreationDate, 1, 4),\n",
    "substr(CreationDate, 1, 7)\n",
    "FROM posts_sample_external\n",
    "WHERE Id is not NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! hive -S -f table-creater.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting query.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query.hql\n",
    "\n",
    "USE joyvan;\n",
    "WITH all_partitions as (\n",
    "SELECT ROW_NUMBER() OVER () as row_num, year, month,  count(*) as num_registers from posts_sample group by year, month\n",
    ")\n",
    "SELECT year, month, num_registers from all_partitions where row_num = 99;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in jar:file:/usr/local/apache-hive-1.1.0-bin/lib/hive-common-1.1.0.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.21 seconds\n",
      "Query ID = jovyan_20180701183737_1a944c78-fc2f-4f8c-a244-fc11b5a982ae\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1530461112798_0025, Tracking URL = http://7f802f72479b:8088/proxy/application_1530461112798_0025/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1530461112798_0025\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2018-07-01 18:37:40,075 Stage-1 map = 0%,  reduce = 0%\n",
      "2018-07-01 18:37:51,554 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.66 sec\n",
      "2018-07-01 18:38:03,941 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.76 sec\n",
      "MapReduce Total cumulative CPU time: 9 seconds 760 msec\n",
      "Ended Job = job_1530461112798_0025\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1530461112798_0026, Tracking URL = http://7f802f72479b:8088/proxy/application_1530461112798_0026/\n",
      "Kill Command = /opt/hadoop/bin/hadoop job  -kill job_1530461112798_0026\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2018-07-01 18:38:23,646 Stage-2 map = 0%,  reduce = 0%\n",
      "2018-07-01 18:38:33,096 Stage-2 map = 100%,  reduce = 0%\n",
      "2018-07-01 18:38:45,330 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 8.64 sec\n",
      "MapReduce Total cumulative CPU time: 8 seconds 640 msec\n",
      "Ended Job = job_1530461112798_0026\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 9.76 sec   HDFS Read: 2937222 HDFS Write: 3206 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 8.64 sec   HDFS Read: 9921 HDFS Write: 16 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 18 seconds 400 msec\n",
      "OK\n",
      "2008\t2008-10\t73\n",
      "Time taken: 90.769 seconds, Fetched: 1 row(s)\n"
     ]
    }
   ],
   "source": [
    "! hive -f query.hql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
